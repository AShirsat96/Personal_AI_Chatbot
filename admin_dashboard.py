import os
import time
import requests
import streamlit as st
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import base64
from io import BytesIO
from PIL import Image
import PyPDF2
import docx
import pickle

# Core libraries
import pandas as pd
from bs4 import BeautifulSoup

# OpenAI for chat
import openai
from openai import OpenAI

# Environment setup
from dotenv import load_dotenv
load_dotenv()

# GitHub Gist Database Class (enhanced)
class GitHubGistDatabase:
    """Free shared database using GitHub Gist"""
    
    def __init__(self):
        # Get credentials from Streamlit secrets
        self.github_token = st.secrets.get("GITHUB_TOKEN", "")
        self.gist_id = st.secrets.get("GIST_ID", "")
        
        if not self.github_token or not self.gist_id:
            self.use_gist = False
        else:
            self.use_gist = True
            
        self.headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
    
    def _load_gist_data(self) -> Dict:
        """Load current data from GitHub Gist"""
        if not self.use_gist:
            return self._get_local_data()
            
        try:
            response = requests.get(
                f"https://api.github.com/gists/{self.gist_id}",
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                gist_data = response.json()
                if "chatbot_data.json" in gist_data["files"]:
                    content = gist_data["files"]["chatbot_data.json"]["content"]
                    return json.loads(content)
                else:
                    return self._get_default_data()
            else:
                return self._get_default_data()
                
        except Exception as e:
            st.warning(f"Error loading gist data: {str(e)}")
            return self._get_local_data()
    
    def _save_gist_data(self, data: Dict) -> bool:
        """Save data to GitHub Gist"""
        if not self.use_gist:
            return self._save_local_data(data)
            
        try:
            payload = {
                "files": {
                    "chatbot_data.json": {
                        "content": json.dumps(data, indent=2, default=str)
                    }
                }
            }
            
            response = requests.patch(
                f"https://api.github.com/gists/{self.gist_id}",
                headers=self.headers,
                json=payload,
                timeout=10
            )
            
            return response.status_code == 200
            
        except Exception as e:
            st.error(f"Error saving to gist: {str(e)}")
            return self._save_local_data(data)
    
    def _get_default_data(self) -> Dict:
        """Get default data structure"""
        return {
            "user_interactions": [],
            "conversations": [],
            "conversation_threads": [],
            "resume_content": None,
            "avatar_data": None,
            "app_settings": {},
            "last_updated": datetime.now().isoformat()
        }
    
    def _get_local_data(self) -> Dict:
        """Fallback to session state storage"""
        if "gist_data" not in st.session_state:
            st.session_state.gist_data = self._get_default_data()
        return st.session_state.gist_data
    
    def _save_local_data(self, data: Dict) -> bool:
        """Save to session state as fallback"""
        st.session_state.gist_data = data
        return True
    
    def save_user_interaction(self, name: str, email: str, session_id: str) -> bool:
        """Save user interaction"""
        try:
            data = self._load_gist_data()
            
            user_entry = {
                "timestamp": datetime.now().isoformat(),
                "name": name,
                "email": email,
                "session_id": session_id
            }
            
            data["user_interactions"].append(user_entry)
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error saving user interaction: {str(e)}")
            return False
    
    def get_user_interactions(self) -> pd.DataFrame:
        """Get all user interactions"""
        try:
            data = self._load_gist_data()
            interactions = data.get("user_interactions", [])
            
            if interactions:
                return pd.DataFrame(interactions)
            else:
                return pd.DataFrame(columns=['timestamp', 'name', 'email', 'session_id'])
                
        except Exception as e:
            st.error(f"Error loading user interactions: {str(e)}")
            return pd.DataFrame(columns=['timestamp', 'name', 'email', 'session_id'])
    
    def get_conversations(self) -> pd.DataFrame:
        """Get all conversation data"""
        try:
            data = self._load_gist_data()
            conversations = data.get("conversations", [])
            
            if conversations:
                return pd.DataFrame(conversations)
            else:
                return pd.DataFrame(columns=[
                    'timestamp', 'session_id', 'user_name', 'user_email', 
                    'user_message', 'bot_response', 'detected_intent', 
                    'response_length', 'message_length'
                ])
                
        except Exception as e:
            st.error(f"Error loading conversations: {str(e)}")
            return pd.DataFrame(columns=[
                'timestamp', 'session_id', 'user_name', 'user_email', 
                'user_message', 'bot_response', 'detected_intent', 
                'response_length', 'message_length'
            ])
    
    def get_conversation_threads(self) -> pd.DataFrame:
        """Get all complete conversation threads"""
        try:
            data = self._load_gist_data()
            threads = data.get("conversation_threads", [])
            
            if threads:
                thread_summaries = []
                for thread in threads:
                    summary = {
                        'session_id': thread['session_id'],
                        'user_name': thread['user_name'],
                        'user_email': thread['user_email'],
                        'start_time': thread['start_time'],
                        'end_time': thread['end_time'],
                        'total_messages': thread['total_messages'],
                        'duration_minutes': self.calculate_conversation_duration(thread['start_time'], thread['end_time']),
                        'saved_at': thread['saved_at']
                    }
                    thread_summaries.append(summary)
                
                return pd.DataFrame(thread_summaries)
            else:
                return pd.DataFrame(columns=[
                    'session_id', 'user_name', 'user_email', 'start_time', 
                    'end_time', 'total_messages', 'duration_minutes', 'saved_at'
                ])
                
        except Exception as e:
            st.error(f"Error loading conversation threads: {str(e)}")
            return pd.DataFrame()
    
    def calculate_conversation_duration(self, start_time: str, end_time: str) -> float:
        """Calculate conversation duration in minutes"""
        try:
            start = pd.to_datetime(start_time)
            end = pd.to_datetime(end_time)
            duration = (end - start).total_seconds() / 60
            return round(duration, 1)
        except:
            return 0.0
    
    def get_complete_conversation(self, session_id: str) -> dict:
        """Get complete conversation thread by session ID"""
        try:
            data = self._load_gist_data()
            threads = data.get("conversation_threads", [])
            
            for thread in threads:
                if thread['session_id'] == session_id:
                    return thread
            
            return None
            
        except Exception as e:
            st.error(f"Error loading conversation: {str(e)}")
            return None
    
    def save_conversation_thread(self, session_id: str, user_name: str, user_email: str, conversation_messages: list) -> bool:
        """Save complete conversation thread"""
        try:
            data = self._load_gist_data()
            
            conversation_thread = {
                "session_id": session_id,
                "user_name": user_name,
                "user_email": user_email,
                "start_time": conversation_messages[0]['timestamp'] if conversation_messages else datetime.now().isoformat(),
                "end_time": conversation_messages[-1]['timestamp'] if conversation_messages else datetime.now().isoformat(),
                "total_messages": len(conversation_messages),
                "conversation_flow": conversation_messages,
                "saved_at": datetime.now().isoformat()
            }
            
            if "conversation_threads" not in data:
                data["conversation_threads"] = []
            
            data["conversation_threads"].append(conversation_thread)
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error saving conversation thread: {str(e)}")
            return False
    
    def save_resume(self, filename: str, content: str, file_type: str, metadata: dict) -> bool:
        """Save resume content"""
        try:
            data = self._load_gist_data()
            
            resume_data = {
                "filename": filename,
                "content": content,
                "file_type": file_type,
                "metadata": metadata,
                "uploaded_at": datetime.now().isoformat()
            }
            
            data["resume_content"] = resume_data
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error saving resume: {str(e)}")
            return False
    
    def get_resume(self) -> Optional[Dict]:
        """Get current resume"""
        try:
            data = self._load_gist_data()
            return data.get("resume_content")
            
        except Exception as e:
            st.error(f"Error loading resume: {str(e)}")
            return None
    
    def delete_resume(self) -> bool:
        """Delete current resume"""
        try:
            data = self._load_gist_data()
            data["resume_content"] = None
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error deleting resume: {str(e)}")
            return False
    
    def save_avatar(self, avatar_base64: str) -> bool:
        """Save avatar data"""
        try:
            data = self._load_gist_data()
            
            avatar_data = {
                "avatar_base64": avatar_base64,
                "uploaded_at": datetime.now().isoformat()
            }
            
            data["avatar_data"] = avatar_data
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error saving avatar: {str(e)}")
            return False
    
    def get_avatar(self) -> Optional[str]:
        """Get current avatar"""
        try:
            data = self._load_gist_data()
            avatar_data = data.get("avatar_data")
            
            if avatar_data:
                return avatar_data.get("avatar_base64")
            return None
            
        except Exception as e:
            st.error(f"Error loading avatar: {str(e)}")
            return None
    
    def delete_avatar(self) -> bool:
        """Delete current avatar"""
        try:
            data = self._load_gist_data()
            data["avatar_data"] = None
            data["last_updated"] = datetime.now().isoformat()
            
            return self._save_gist_data(data)
            
        except Exception as e:
            st.error(f"Error deleting avatar: {str(e)}")
            return False
    
    def get_database_status(self) -> Dict:
        """Get database connection status"""
        status = {
            "database_type": "GitHub Gist" if self.use_gist else "Local Session",
            "connected": self.use_gist,
            "gist_configured": bool(self.github_token and self.gist_id)
        }
        
        if self.use_gist:
            try:
                response = requests.get(f"https://api.github.com/gists/{self.gist_id}", headers=self.headers)
                status["connection_test"] = response.status_code == 200
                status["last_updated"] = self._load_gist_data().get("last_updated", "Never")
            except:
                status["connection_test"] = False
        
        return status
    
    def export_all_data(self) -> Dict:
        """Export all data for backup"""
        data = self._load_gist_data()
        data["export_timestamp"] = datetime.now().isoformat()
        return data
    
    def clear_all_data(self) -> bool:
        """Clear all data (danger zone)"""
        try:
            default_data = self._get_default_data()
            return self._save_gist_data(default_data)
        except Exception as e:
            st.error(f"Error clearing data: {str(e)}")
            return False

# Initialize shared database
@st.cache_resource
def get_shared_db():
    """Get shared database instance"""
    return GitHubGistDatabase()

# Updated storage functions
def save_user_info_shared(name: str, email: str, session_id: str) -> bool:
    """Save user info to shared database"""
    db = get_shared_db()
    return db.save_user_interaction(name, email, session_id)

def load_user_data_shared() -> pd.DataFrame:
    """Load user data from shared database"""
    db = get_shared_db()
    return db.get_user_interactions()

def load_conversation_data_shared() -> pd.DataFrame:
    """Load conversation data from shared database"""
    db = get_shared_db()
    return db.get_conversations()

def load_conversation_threads_shared() -> pd.DataFrame:
    """Load conversation threads from shared database"""
    db = get_shared_db()
    return db.get_conversation_threads()

def get_complete_conversation_shared(session_id: str) -> dict:
    """Get complete conversation thread"""
    db = get_shared_db()
    return db.get_complete_conversation(session_id)

def save_conversation_thread_shared(session_id: str, user_name: str, user_email: str, messages: list) -> bool:
    """Save complete conversation thread"""
    db = get_shared_db()
    return db.save_conversation_thread(session_id, user_name, user_email, messages)

def analyze_intent_patterns(df: pd.DataFrame) -> Dict:
    """Analyze intent patterns from conversation data"""
    if df.empty:
        return {}
    
    intent_counts = df['detected_intent'].value_counts().to_dict()
    
    # Calculate response efficiency
    avg_response_length = df.groupby('detected_intent')['response_length'].mean().to_dict()
    
    # Find most common questions
    common_questions = df['user_message'].value_counts().head(10).to_dict()
    
    return {
        'intent_distribution': intent_counts,
        'avg_response_length': avg_response_length,
        'common_questions': common_questions,
        'total_conversations': len(df)
    }

def save_avatar_shared(avatar_base64: str) -> bool:
    """Save avatar to shared database"""
    db = get_shared_db()
    return db.save_avatar(avatar_base64)

def load_avatar_shared() -> Optional[str]:
    """Load avatar from shared database"""
    db = get_shared_db()
    return db.get_avatar()

def delete_avatar_shared() -> bool:
    """Delete avatar from shared database"""
    db = get_shared_db()
    return db.delete_avatar()

def save_resume_shared(filename: str, content: str, file_type: str, metadata: dict) -> bool:
    """Save resume to shared database"""
    db = get_shared_db()
    return db.save_resume(filename, content, file_type, metadata)

def load_resume_shared() -> Optional[Dict]:
    """Load resume from shared database"""
    db = get_shared_db()
    return db.get_resume()

def delete_resume_shared() -> bool:
    """Delete resume from shared database"""
    db = get_shared_db()
    return db.delete_resume()

def export_user_data_shared():
    """Export user data from shared database"""
    db = get_shared_db()
    df = db.get_user_interactions()
    if not df.empty:
        return df.to_csv(index=False)
    return None

def show_database_status():
    """Show database connection status"""
    db = get_shared_db()
    status = db.get_database_status()
    
    if status["connected"]:
        st.success(f"‚úÖ Connected to {status['database_type']}")
        st.info(f"Last updated: {status.get('last_updated', 'Unknown')}")
    else:
        st.warning(f"‚ö†Ô∏è Using {status['database_type']} (limited functionality)")
        st.info("Configure GitHub Gist for full app synchronization")
    
    return status

# Original classes with minimal changes
@dataclass
class WebsiteContent:
    """Data class for storing scraped website content"""
    url: str
    title: str
    content: str
    metadata: Dict
    timestamp: datetime

@dataclass
class ResumeContent:
    """Data class for storing resume content"""
    filename: str
    content: str
    file_type: str
    metadata: Dict
    timestamp: datetime

class AniketChatbotAI:
    """Professional assistant for Aniket Shirsat's portfolio"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

def get_image_base64(image_file):
    """Convert uploaded image to base64 string"""
    try:
        img = Image.open(image_file)
        img = img.resize((100, 100), Image.Resampling.LANCZOS)
        buffered = BytesIO()
        img.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return f"data:image/png;base64,{img_str}"
    except Exception as e:
        st.error(f"Error processing avatar image: {str(e)}")
        return None

def calculate_conversation_duration(start_time: str, end_time: str) -> float:
    """Calculate conversation duration in minutes"""
    try:
        start = pd.to_datetime(start_time)
        end = pd.to_datetime(end_time)
        duration = (end - start).total_seconds() / 60
        return round(duration, 1)
    except:
        return 0.0

class SimpleWebsiteScraper:
    """Simplified website scraper using only requests and BeautifulSoup"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def extract_links(self, base_url: str, soup: BeautifulSoup) -> List[str]:
        """Extract all internal links from a page"""
        links = set()
        domain = urlparse(base_url).netloc
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            
            if urlparse(full_url).netloc == domain:
                links.add(full_url)
                
        return list(links)
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize extracted text"""
        text = ' '.join(text.split())
        return text.strip()
    
    def scrape_page(self, url: str) -> Optional[WebsiteContent]:
        """Scrape a single page and extract content"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            title = soup.find('title')
            title_text = title.get_text().strip() if title else url
            
            content_selectors = [
                'main', 'article', '.content', '#content', 
                '.post', '.entry-content', 'section'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text() for elem in elements])
                    break
            
            if not content:
                body = soup.find('body')
                if body:
                    content = body.get_text()
            
            content = self.clean_text(content)
            
            metadata = {
                'word_count': len(content.split()),
                'scraped_at': datetime.now().isoformat(),
            }
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                metadata['description'] = meta_desc.get('content', '')
            
            return WebsiteContent(
                url=url,
                title=title_text,
                content=content,
                metadata=metadata,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            st.error(f"Error scraping {url}: {str(e)}")
            return None
    
    def scrape_website(self, base_url: str, max_pages: int = 10) -> List[WebsiteContent]:
        """Scrape an entire website starting from base URL"""
        scraped_content = []
        visited_urls = set()
        urls_to_visit = [base_url]
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        while urls_to_visit and len(scraped_content) < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
                
            visited_urls.add(current_url)
            
            status_text.text(f"Analyzing: {current_url}")
            progress = len(scraped_content) / max_pages
            progress_bar.progress(min(progress, 1.0))
            
            content = self.scrape_page(current_url)
            
            if content and len(content.content) > 100:
                scraped_content.append(content)
                
                if len(scraped_content) < max_pages:
                    try:
                        response = self.session.get(current_url, timeout=5)
                        soup = BeautifulSoup(response.text, 'html.parser')
                        new_links = self.extract_links(base_url, soup)
                        
                        for link in new_links:
                            if link not in visited_urls and link not in urls_to_visit:
                                urls_to_visit.append(link)
                                
                    except Exception as e:
                        st.warning(f"Could not extract links from {current_url}")
            
            time.sleep(1)
        
        progress_bar.progress(1.0)
        status_text.text(f"Analysis complete! Found {len(scraped_content)} pages.")
        
        return scraped_content

class ResumeProcessor:
    """Process and extract content from resume files"""
    
    @staticmethod
    def extract_text_from_pdf(file_bytes: bytes) -> str:
        """Extract text from PDF file"""
        try:
            pdf_file = BytesIO(file_bytes)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            
            return text.strip()
        except Exception as e:
            st.error(f"Error reading PDF: {str(e)}")
            return ""
    
    @staticmethod
    def extract_text_from_docx(file_bytes: bytes) -> str:
        """Extract text from DOCX file"""
        try:
            docx_file = BytesIO(file_bytes)
            doc = docx.Document(docx_file)
            
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            return text.strip()
        except Exception as e:
            st.error(f"Error reading DOCX: {str(e)}")
            return ""
    
    @staticmethod
    def extract_text_from_txt(file_bytes: bytes) -> str:
        """Extract text from TXT file"""
        try:
            return file_bytes.decode('utf-8').strip()
        except Exception as e:
            st.error(f"Error reading TXT: {str(e)}")
            return ""
    
    def process_resume(self, uploaded_file) -> Optional[ResumeContent]:
        """Process uploaded resume file and extract content"""
        try:
            file_bytes = uploaded_file.read()
            file_type = uploaded_file.type
            filename = uploaded_file.name
            
            if file_type == "application/pdf":
                content = self.extract_text_from_pdf(file_bytes)
            elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                content = self.extract_text_from_docx(file_bytes)
            elif file_type == "text/plain":
                content = self.extract_text_from_txt(file_bytes)
            else:
                st.error(f"Unsupported file type: {file_type}")
                return None
            
            if not content:
                st.error("Could not extract text from the file")
                return None
            
            metadata = {
                'word_count': len(content.split()),
                'file_size': len(file_bytes),
                'processed_at': datetime.now().isoformat(),
            }
            
            return ResumeContent(
                filename=filename,
                content=content,
                file_type=file_type,
                metadata=metadata,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            st.error(f"Error processing resume: {str(e)}")
            return None

def display_complete_conversation(conversation_thread: dict):
    """Display a complete conversation thread in chat format"""
    st.markdown(f"""
    ### üí¨ Complete Conversation
    **User:** {conversation_thread['user_name']} ({conversation_thread['user_email']})  
    **Session:** {conversation_thread['session_id']}  
    **Duration:** {calculate_conversation_duration(conversation_thread['start_time'], conversation_thread['end_time'])} minutes  
    **Total Messages:** {conversation_thread['total_messages']}
    """)
    
    st.markdown("---")
    
    # Display conversation flow
    messages = conversation_thread['conversation_flow']
    
    for i, message in enumerate(messages):
        timestamp = pd.to_datetime(message['timestamp']).strftime('%H:%M:%S')
        
        if message['role'] == 'user':
            # User message
            st.markdown(f"""
            <div style="
                display: flex; 
                justify-content: flex-end; 
                margin: 10px 0;
            ">
                <div style="
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 12px 16px;
                    border-radius: 18px 18px 4px 18px;
                    max-width: 70%;
                    margin-left: 30%;
                ">
                    <div style="font-size: 12px; opacity: 0.8; margin-bottom: 4px;">
                        {conversation_thread['user_name']} ‚Ä¢ {timestamp}
                    </div>
                    {message['content']}
                </div>
            </div>
            """, unsafe_allow_html=True)
        
        else:
            # Bot message
            intent_badge = ""
            if 'intent' in message:
                intent_colors = {
                    'hiring': '#e74c3c', 'skills': '#3498db', 'projects': '#f39c12',
                    'education': '#2ecc71', 'personal': '#9b59b6', 'contact': '#e67e22',
                    'general': '#95a5a6'
                }
                color = intent_colors.get(message['intent'], '#95a5a6')
                intent_badge = f"""
                <span style="
                    background: {color}; 
                    color: white; 
                    padding: 2px 6px; 
                    border-radius: 8px; 
                    font-size: 10px;
                    margin-left: 8px;
                ">
                    {message['intent'].upper()}
                </span>
                """
            
            st.markdown(f"""
            <div style="
                display: flex; 
                align-items: flex-start; 
                margin: 10px 0;
            ">
                <div style="
                    width: 35px; 
                    height: 35px; 
                    background: #667eea; 
                    border-radius: 50%; 
                    display: flex; 
                    align-items: center; 
                    justify-content: center; 
                    color: white; 
                    font-size: 18px;
                    margin-right: 10px;
                ">ü§ñ</div>
                <div style="
                    background: #f1f3f5;
                    padding: 12px 16px;
                    border-radius: 18px 18px 18px 4px;
                    max-width: 70%;
                ">
                    <div style="font-size: 12px; color: #666; margin-bottom: 4px;">
                        Aniket's AI Assistant ‚Ä¢ {timestamp} {intent_badge}
                    </div>
                    {message['content']}
                </div>
            </div>
            """, unsafe_allow_html=True)

def enhanced_conversation_analysis():
    """Enhanced conversation analysis with more detailed insights"""
    conversation_data = load_conversation_data_shared()
    
    if conversation_data.empty:
        st.info("No conversation data available yet.")
        return
    
    # Convert timestamp
    conversation_data['timestamp'] = pd.to_datetime(conversation_data['timestamp'], errors='coerce')
    
    # Add derived columns for analysis
    conversation_data['hour'] = conversation_data['timestamp'].dt.hour
    conversation_data['day_of_week'] = conversation_data['timestamp'].dt.day_name()
    conversation_data['date'] = conversation_data['timestamp'].dt.date
    
    # Advanced metrics
    st.subheader("üìä Advanced Conversation Analytics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_sessions = conversation_data['session_id'].nunique()
        st.metric("Unique Sessions", total_sessions)
    
    with col2:
        avg_messages_per_session = conversation_data.groupby('session_id').size().mean()
        st.metric("Avg Messages/Session", f"{avg_messages_per_session:.1f}")
    
    with col3:
        avg_response_length = conversation_data['response_length'].mean()
        st.metric("Avg Response Length", f"{avg_response_length:.0f} chars")
    
    with col4:
        total_unique_users = conversation_data['user_email'].nunique()
        st.metric("Unique Users", total_unique_users)
    
    # Time-based analysis
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üïê Activity by Hour")
        hourly_activity = conversation_data['hour'].value_counts().sort_index()
        st.bar_chart(hourly_activity)
    
    with col2:
        st.subheader("üìÖ Activity by Day")
        daily_activity = conversation_data['day_of_week'].value_counts()
        st.bar_chart(daily_activity)
    
    # Intent analysis over time
    st.subheader("üéØ Intent Trends Over Time")
    intent_over_time = conversation_data.groupby(['date', 'detected_intent']).size().unstack(fill_value=0)
    st.line_chart(intent_over_time)

def conversation_search_and_filter():
    """Advanced search and filtering for conversations - FIXED VERSION"""
    st.subheader("üîç Conversation Search & Filter")
    
    conversation_data = load_conversation_data_shared()
    
    if conversation_data.empty:
        st.info("No conversation data to search.")
        return
    
    # Convert timestamp FIRST and handle any errors
    conversation_data['timestamp'] = pd.to_datetime(conversation_data['timestamp'], errors='coerce')
    
    # Drop rows with invalid timestamps
    conversation_data = conversation_data.dropna(subset=['timestamp'])
    
    if conversation_data.empty:
        st.warning("No valid conversation data found after timestamp parsing.")
        return
    
    # Filters
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Intent filter
        unique_intents = ['All'] + sorted(list(conversation_data['detected_intent'].dropna().unique()))
        selected_intent = st.selectbox("Filter by Intent", unique_intents)
    
    with col2:
        # Date filter - FIXED IMPLEMENTATION
        min_date = conversation_data['timestamp'].min().date()
        max_date = conversation_data['timestamp'].max().date()
        
        # Use separate date inputs with toggle
        date_filter_enabled = st.checkbox("Enable Date Filter", value=False)
        
        if date_filter_enabled:
            col2a, col2b = st.columns(2)
            with col2a:
                start_date = st.date_input(
                    "From",
                    value=min_date,
                    min_value=min_date,
                    max_value=max_date,
                    key="filter_start_date"
                )
            with col2b:
                end_date = st.date_input(
                    "To", 
                    value=max_date,
                    min_value=min_date,
                    max_value=max_date,
                    key="filter_end_date"
                )
            
            # Validate date range
            if start_date > end_date:
                st.error("Start date cannot be after end date!")
                return
        else:
            start_date = min_date
            end_date = max_date
    
    with col3:
        # User filter
        unique_users = ['All'] + sorted(list(conversation_data['user_name'].dropna().unique()))
        selected_user = st.selectbox("Filter by User", unique_users)
    
    # Search box
    search_term = st.text_input("üîç Search in messages", placeholder="Enter keywords to search...")
    
    # Apply filters step by step with debugging
    filtered_data = conversation_data.copy()
    
    # Debug: Show initial count
    st.write(f"**Initial conversations:** {len(filtered_data)}")
    
    # Intent filter
    if selected_intent != 'All':
        before_count = len(filtered_data)
        filtered_data = filtered_data[filtered_data['detected_intent'] == selected_intent]
        st.write(f"**After intent filter ({selected_intent}):** {len(filtered_data)} (removed {before_count - len(filtered_data)})")
    
    # Date filter - FIXED IMPLEMENTATION
    if date_filter_enabled:
        before_count = len(filtered_data)
        
        # Convert dates to datetime for comparison
        start_datetime = pd.Timestamp(start_date)
        end_datetime = pd.Timestamp(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # End of day
        
        filtered_data = filtered_data[
            (filtered_data['timestamp'] >= start_datetime) & 
            (filtered_data['timestamp'] <= end_datetime)
        ]
        st.write(f"**After date filter ({start_date} to {end_date}):** {len(filtered_data)} (removed {before_count - len(filtered_data)})")
    
    # User filter
    if selected_user != 'All':
        before_count = len(filtered_data)
        filtered_data = filtered_data[filtered_data['user_name'] == selected_user]
        st.write(f"**After user filter ({selected_user}):** {len(filtered_data)} (removed {before_count - len(filtered_data)})")
    
    # Search term filter
    if search_term:
        before_count = len(filtered_data)
        search_mask = (
            filtered_data['user_message'].str.contains(search_term, case=False, na=False) |
            filtered_data['bot_response'].str.contains(search_term, case=False, na=False)
        )
        filtered_data = filtered_data[search_mask]
        st.write(f"**After search filter ('{search_term}'):** {len(filtered_data)} (removed {before_count - len(filtered_data)})")
    
    # Final results
    st.markdown("---")
    st.write(f"**üìä Final Results: {len(filtered_data)} conversations found**")
    
    if not filtered_data.empty:
        # Add export button for filtered results
        col1, col2 = st.columns([3, 1])
        
        with col2:
            filtered_csv = filtered_data.to_csv(index=False)
            st.download_button(
                label="üì• Export Filtered Results",
                data=filtered_csv,
                file_name=f"filtered_conversations_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv"
            )
        
        # Display conversations with improved formatting
        st.subheader("üí¨ Conversation Results")
        
        # Pagination
        items_per_page = 10
        total_pages = max(1, (len(filtered_data) + items_per_page - 1) // items_per_page)
        
        if total_pages > 1:
            page = st.selectbox("Page", range(1, total_pages + 1)) - 1
        else:
            page = 0
        
        start_idx = page * items_per_page
        end_idx = min(start_idx + items_per_page, len(filtered_data))
        
        page_data = filtered_data.sort_values('timestamp', ascending=False).iloc[start_idx:end_idx]
        
        for idx, (_, conv) in enumerate(page_data.iterrows()):
            # Color coding by intent
            intent_colors = {
                'hiring': '#e74c3c',
                'skills': '#3498db', 
                'projects': '#f39c12',
                'education': '#2ecc71',
                'personal': '#9b59b6',
                'contact': '#e67e22',
                'general': '#95a5a6'
            }
            intent_color = intent_colors.get(conv['detected_intent'], '#95a5a6')
            
            with st.expander(
                f"üïê {conv['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} - {conv['user_name']} "
                f"({conv['detected_intent']}) - Session: {conv['session_id'][:8]}..."
            ):
                # Highlight search terms if any
                user_msg = conv['user_message']
                bot_msg = conv['bot_response']
                
                if search_term:
                    # Simple highlighting (for display purposes)
                    user_msg = user_msg.replace(search_term, f"**{search_term}**")
                    bot_msg = bot_msg.replace(search_term, f"**{search_term}**")
                
                st.markdown(f"""
                <div style="border-left: 4px solid {intent_color}; padding: 15px; margin: 10px 0; background: #f8f9fa; border-radius: 0 8px 8px 0;">
                    <div style="margin-bottom: 10px;">
                        <strong>üë§ User Message:</strong><br>
                        {user_msg}
                    </div>
                    <div style="margin-bottom: 10px;">
                        <strong>ü§ñ Bot Response:</strong><br>
                        {bot_msg}
                    </div>
                    <div style="font-size: 12px; color: #666; display: flex; justify-content: space-between;">
                        <span><strong>Email:</strong> {conv['user_email']}</span>
                        <span><strong>Response Length:</strong> {conv['response_length']} chars</span>
                        <span><strong>Session:</strong> {conv['session_id']}</span>
                    </div>
                </div>
                """, unsafe_allow_html=True)
        
        # Show pagination info
        if total_pages > 1:
            st.write(f"Showing {start_idx + 1}-{end_idx} of {len(filtered_data)} conversations (Page {page + 1} of {total_pages})")
    
    else:
        st.info("üîç No conversations match your filter criteria. Try adjusting your filters.")
        
        # Suggestions for no results
        st.markdown("""
        **üí° Try these suggestions:**
        - Remove or change the date filter
        - Select 'All' for intent or user filters  
        - Use broader search terms
        - Check if there's data in the selected date range
        """)

def live_conversation_monitor():
    """Real-time conversation monitoring with auto-refresh"""
    st.subheader("üì° Live Conversation Monitor")
    
    # Auto-refresh controls
    col1, col2, col3 = st.columns(3)
    
    with col1:
        auto_refresh = st.checkbox("üîÑ Auto-refresh every 10 seconds")
    
    with col2:
        if st.button("üîÑ Manual Refresh"):
            st.rerun()
    
    with col3:
        sound_alerts = st.checkbox("üîî Sound alerts (coming soon)")
    
    # Auto-refresh logic
    if auto_refresh:
        time.sleep(10)
        st.rerun()
    
    conversation_data = load_conversation_data_shared()
    
    if conversation_data.empty:
        st.info("üéØ Monitoring active - waiting for conversations...")
        st.markdown("**Test the connection:**")
        st.markdown("1. Open your chatbot widget in another tab")
        st.markdown("2. Send a test message")
        st.markdown("3. Watch it appear here in real-time!")
        return
    
    # Convert timestamp
    conversation_data['timestamp'] = pd.to_datetime(conversation_data['timestamp'], errors='coerce')
    
    # Show recent activity (last 24 hours)
    recent_cutoff = datetime.now() - pd.Timedelta(hours=24)
    recent_data = conversation_data[conversation_data['timestamp'] > recent_cutoff]
    
    # Real-time stats
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üïê Last 24h Messages", len(recent_data))
    
    with col2:
        active_sessions_24h = recent_data['session_id'].nunique()
        st.metric("üë• Active Sessions (24h)", active_sessions_24h)
    
    with col3:
        if not recent_data.empty:
            last_activity = recent_data['timestamp'].max()
            time_since = datetime.now() - last_activity.replace(tzinfo=None)
            minutes_ago = int(time_since.total_seconds() / 60)
            st.metric("‚è±Ô∏è Last Activity", f"{minutes_ago}m ago")
        else:
            st.metric("‚è±Ô∏è Last Activity", "No recent activity")
    
    with col4:
        if not recent_data.empty:
            top_intent_24h = recent_data['detected_intent'].mode().iloc[0] if not recent_data['detected_intent'].mode().empty else "N/A"
            st.metric("üéØ Top Intent (24h)", top_intent_24h)
        else:
            st.metric("üéØ Top Intent (24h)", "N/A")
    
    # Live feed of recent conversations
    st.markdown("---")
    st.markdown("**üì± Live Conversation Feed (Last 20 messages)**")
    
    recent_conversations = conversation_data.sort_values('timestamp', ascending=False).head(20)
    
    for i, (_, conv) in enumerate(recent_conversations.iterrows()):
        # Time since message
        time_since = datetime.now() - conv['timestamp'].replace(tzinfo=None)
        
        # Color coding based on recency
        if time_since.total_seconds() < 300:  # 5 minutes
            border_color = "#28a745"  # Green
            time_indicator = "üü¢ LIVE"
        elif time_since.total_seconds() < 3600:  # 1 hour
            border_color = "#ffc107"  # Yellow
            time_indicator = "üü° Recent"
        else:
            border_color = "#6c757d"  # Gray
            time_indicator = "‚ö™ Older"
        
        # Intent color coding
        intent_colors = {
            'hiring': '#e74c3c',
            'skills': '#3498db',
            'projects': '#f39c12',
            'education': '#2ecc71',
            'personal': '#9b59b6',
            'contact': '#e67e22',
            'general': '#95a5a6'
        }
        intent_color = intent_colors.get(conv['detected_intent'], '#95a5a6')
        
        # Format timestamp
        timestamp_str = conv['timestamp'].strftime('%H:%M:%S')
        
        # Display conversation
        st.markdown(f"""
        <div style="
            border-left: 4px solid {border_color}; 
            padding: 12px; 
            margin: 8px 0; 
            background: #f8f9fa;
            border-radius: 0 8px 8px 0;
        ">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
                <strong>{timestamp_str}</strong> - <strong>{conv['user_name']}</strong>
                <span style="background: {intent_color}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 12px;">
                    {conv['detected_intent'].upper()}
                </span>
                <span style="font-size: 12px; color: #666;">{time_indicator}</span>
            </div>
            <div style="margin: 4px 0;">
                <strong>üë§ Q:</strong> {conv['user_message'][:100]}{'...' if len(conv['user_message']) > 100 else ''}
            </div>
            <div style="color: #666; font-size: 14px;">
                <strong>ü§ñ A:</strong> {conv['bot_response'][:150]}{'...' if len(conv['bot_response']) > 150 else ''}
            </div>
        </div>
        """, unsafe_allow_html=True)

def conversation_export_options():
    """Enhanced export options for conversation data"""
    st.subheader("üì• Advanced Export Options")
    
    conversation_data = load_conversation_data_shared()
    
    if conversation_data.empty:
        st.info("No conversation data to export.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**üìä Standard Exports**")
        
        # Full conversation export
        conv_csv = conversation_data.to_csv(index=False)
        st.download_button(
            label="üì• Complete Conversations (CSV)",
            data=conv_csv,
            file_name=f"conversations_complete_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv"
        )
        
        # JSON export for backup
        conv_json = conversation_data.to_json(orient='records', date_format='iso')
        st.download_button(
            label="üì• Complete Conversations (JSON)",
            data=conv_json,
            file_name=f"conversations_backup_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
            mime="application/json"
        )
    
    with col2:
        st.markdown("**üìà Analytics Exports**")
        
        # Intent summary
        intent_summary = conversation_data.groupby('detected_intent').agg({
            'user_message': 'count',
            'response_length': 'mean',
            'message_length': 'mean'
        }).round(2)
        intent_summary.columns = ['Total_Questions', 'Avg_Response_Length', 'Avg_Question_Length']
        
        intent_csv = intent_summary.to_csv()
        st.download_button(
            label="üìä Intent Analytics (CSV)",
            data=intent_csv,
            file_name=f"intent_analytics_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )
        
        # User engagement summary
        user_summary = conversation_data.groupby('user_email').agg({
            'user_message': 'count',
            'session_id': 'nunique',
            'detected_intent': lambda x: len(x.unique()),
            'timestamp': ['min', 'max']
        }).round(2)
        
        user_csv = user_summary.to_csv()
        st.download_button(
            label="üë• User Engagement (CSV)",
            data=user_csv,
            file_name=f"user_engagement_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )

def conversation_threads_tab():
    """Complete conversation threads management tab"""
    st.header("üí¨ Complete Conversation Threads")
    
    # Load conversation threads
    threads_df = load_conversation_threads_shared()
    
    if threads_df.empty:
        st.info("üìù No complete conversation threads saved yet.")
        st.markdown("""
        **How conversation threads work:**
        1. Each user session creates a conversation thread
        2. All messages in that session are saved together
        3. You can view the complete conversation flow
        4. Threads are automatically saved when sessions end
        """)
        return
    
    # Convert timestamps for display
    threads_df['start_time'] = pd.to_datetime(threads_df['start_time'])
    threads_df['end_time'] = pd.to_datetime(threads_df['end_time'])
    
    # Summary statistics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_threads = len(threads_df)
        st.metric("Total Conversations", total_threads)
    
    with col2:
        avg_messages = threads_df['total_messages'].mean()
        st.metric("Avg Messages/Conversation", f"{avg_messages:.1f}")
    
    with col3:
        avg_duration = threads_df['duration_minutes'].mean()
        st.metric("Avg Duration", f"{avg_duration:.1f}m")
    
    with col4:
        total_messages = threads_df['total_messages'].sum()
        st.metric("Total Messages", total_messages)
    
    # Filters
    st.subheader("üîç Filter Conversations")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # User filter
        unique_users = ['All'] + list(threads_df['user_name'].unique())
        selected_user = st.selectbox("Filter by User", unique_users)
    
    with col2:
        # Date filter
        min_date = threads_df['start_time'].min().date()
        max_date = threads_df['start_time'].max().date()
        date_range = st.date_input(
            "Date Range",
            value=(min_date, max_date),
            min_value=min_date,
            max_value=max_date
        )
    
    with col3:
        # Duration filter
        min_duration = st.number_input("Min Duration (minutes)", min_value=0.0, value=0.0)
    
    # Apply filters
    filtered_df = threads_df.copy()
    
    if selected_user != 'All':
        filtered_df = filtered_df[filtered_df['user_name'] == selected_user]
    
    if len(date_range) == 2:
        start_date, end_date = date_range
        filtered_df = filtered_df[
            (filtered_df['start_time'].dt.date >= start_date) & 
            (filtered_df['start_time'].dt.date <= end_date)
        ]
    
    if min_duration > 0:
        filtered_df = filtered_df[filtered_df['duration_minutes'] >= min_duration]
    
    st.write(f"**Showing {len(filtered_df)} conversations**")
    
    # Conversation list
    if not filtered_df.empty:
        # Sort by most recent first
        filtered_df = filtered_df.sort_values('start_time', ascending=False)
        
        for _, thread in filtered_df.iterrows():
            with st.expander(
                f"üí¨ {thread['user_name']} ‚Ä¢ {thread['start_time'].strftime('%Y-%m-%d %H:%M')} ‚Ä¢ "
                f"{thread['total_messages']} messages ‚Ä¢ {thread['duration_minutes']}m"
            ):
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.write(f"**User:** {thread['user_name']} ({thread['user_email']})")
                    st.write(f"**Session ID:** {thread['session_id']}")
                    st.write(f"**Duration:** {thread['duration_minutes']} minutes")
                
                with col2:
                    if st.button(f"üëÅÔ∏è View Full Conversation", key=f"view_{thread['session_id']}"):
                        st.session_state.selected_conversation = thread['session_id']
                
                # Show conversation preview (first and last message)
                conversation = get_complete_conversation_shared(thread['session_id'])
                if conversation and conversation['conversation_flow']:
                    messages = conversation['conversation_flow']
                    
                    st.markdown("**Conversation Preview:**")
                    
                    # First message
                    first_msg = messages[0]
                    if first_msg['role'] == 'user':
                        st.markdown(f"üë§ **First:** {first_msg['content'][:100]}{'...' if len(first_msg['content']) > 100 else ''}")
                    
                    # Last message
                    if len(messages) > 1:
                        last_msg = messages[-1]
                        if last_msg['role'] == 'assistant':
                            st.markdown(f"ü§ñ **Last:** {last_msg['content'][:100]}{'...' if len(last_msg['content']) > 100 else ''}")
    
    # Display selected conversation
    if hasattr(st.session_state, 'selected_conversation'):
        st.markdown("---")
        conversation = get_complete_conversation_shared(st.session_state.selected_conversation)
        if conversation:
            display_complete_conversation(conversation)
            
            if st.button("‚ùå Close Conversation View"):
                del st.session_state.selected_conversation
                st.rerun()

def export_conversation_threads():
    """Export complete conversation threads"""
    st.subheader("üì• Export Complete Conversations")
    
    db = get_shared_db()
    data = db._load_gist_data()
    threads = data.get("conversation_threads", [])
    
    if not threads:
        st.info("No conversation threads to export.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Export all threads as JSON
        threads_json = json.dumps(threads, indent=2, default=str)
        st.download_button(
            label="üì• Export All Conversations (JSON)",
            data=threads_json,
            file_name=f"complete_conversations_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
            mime="application/json",
            help="Download all conversation threads with complete message history"
        )
    
    with col2:
        # Export summary as CSV
        threads_df = load_conversation_threads_shared()
        if not threads_df.empty:
            summary_csv = threads_df.to_csv(index=False)
            st.download_button(
                label="üìä Export Summary (CSV)",
                data=summary_csv,
                file_name=f"conversation_summary_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                help="Download conversation summary statistics"
            )

def enhanced_analytics_tab_v2():
    """Enhanced analytics with all new features"""
    st.header("üìä Enhanced Conversation Analytics")
    
    # Tabs within analytics
    subtab1, subtab2, subtab3, subtab4 = st.tabs([
        "üìà Advanced Analytics", 
        "üîç Search & Filter", 
        "üì• Export Options",
        "üì° Live Monitor"
    ])
    
    with subtab1:
        enhanced_conversation_analysis()
    
    with subtab2:
        conversation_search_and_filter()
    
    with subtab3:
        conversation_export_options()
    
    with subtab4:
        live_conversation_monitor()

def live_monitoring_tab():
    """Live monitoring tab"""
    st.header("üì° Live Chatbot Monitoring")
    
    # Real-time status
    st.subheader("üî¥ Real-time Status")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        db = get_shared_db()
        if db.use_gist:
            st.success("üü¢ Chatbot Connected")
        else:
            st.error("üî¥ Chatbot Disconnected")
    
    with col2:
        conversation_data = load_conversation_data_shared()
        if not conversation_data.empty:
            conversation_data['timestamp'] = pd.to_datetime(conversation_data['timestamp'], errors='coerce')
            last_activity = conversation_data['timestamp'].max()
            if pd.notna(last_activity):
                time_since = datetime.now() - last_activity.replace(tzinfo=None)
                if time_since.total_seconds() < 300:  # 5 minutes
                    st.success(f"üü¢ Active {int(time_since.total_seconds())}s ago")
                else:
                    st.warning(f"üü° Last seen {int(time_since.total_seconds()/60)}m ago")
            else:
                st.info("üü° No recent activity")
        else:
            st.info("üü° No activity recorded")
    
    with col3:
        user_data = load_user_data_shared()
        active_sessions = user_data['session_id'].nunique() if not user_data.empty else 0
        st.info(f"üë• {active_sessions} Total Sessions")
    
    # Auto-refresh toggle
    st.subheader("‚öôÔ∏è Monitoring Settings")
    
    col1, col2 = st.columns(2)
    with col1:
        auto_refresh = st.checkbox("üîÑ Auto-refresh every 10 seconds")
    
    with col2:
        if st.button("üîÑ Manual Refresh"):
            st.rerun()
    
    # Auto-refresh logic
    if auto_refresh:
        time.sleep(10)
        st.rerun()
    
    # Enhanced live monitoring
    enhanced_conversation_analysis()

def process_resume_file(uploaded_file):
    """Process uploaded resume file"""
    with st.spinner("Processing resume..."):
        processor = ResumeProcessor()
        resume_content = processor.process_resume(uploaded_file)
        
        if resume_content:
            # Save to shared database
            success = save_resume_shared(
                resume_content.filename,
                resume_content.content,
                resume_content.file_type,
                resume_content.metadata
            )
            
            if success:
                st.success("‚úÖ Resume saved and synced to chat widget!")
            else:
                st.error("‚ùå Failed to save resume")
                return
            
            # Show processing results
            st.subheader("üìã Resume Processing Results")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Filename", resume_content.filename)
            with col2:
                st.metric("Word Count", resume_content.metadata['word_count'])
            with col3:
                st.metric("File Size", f"{resume_content.metadata['file_size'] / 1024:.1f} KB")
            
            st.subheader("üìÑ Content Preview")
            preview_text = resume_content.content[:500] + "..." if len(resume_content.content) > 500 else resume_content.content
            st.text_area("Extracted Text (Preview)", preview_text, height=200, disabled=True)
            
            st.success("üéâ Resume is now active in the chat widget!")
            
        else:
            st.error("Failed to process the resume. Please check the file format and try again.")

def scrape_website(url: str, max_pages: int):
    """Update knowledge base with website content"""
    with st.spinner("Updating knowledge base..."):
        scraper = SimpleWebsiteScraper()
        content = scraper.scrape_website(url, max_pages)
        
        if content:
            st.subheader("üìä Website Analysis Results")
            df = pd.DataFrame([
                {
                    "Page URL": c.url,
                    "Title": c.title[:50] + "..." if len(c.title) > 50 else c.title,
                    "Content Length": c.metadata.get("word_count", 0),
                    "Scraped At": c.timestamp.strftime("%Y-%m-%d %H:%M")
                }
                for c in content
            ])
            st.dataframe(df, use_container_width=True)
            
            total_words = sum(c.metadata.get("word_count", 0) for c in content)
            st.info(f"üìà Successfully processed {len(content)} pages with {total_words:,} total words")
            
            # Note: Website content would need additional integration with the knowledge base
            # This is mainly for content analysis
            
        else:
            st.error("Could not retrieve content. Please verify the URL and try again.")

def main():
    """Admin Dashboard - Complete management interface"""
    st.set_page_config(
        page_title="Aniket Shirsat - Admin Dashboard",
        page_icon="‚öôÔ∏è",
        layout="wide"
    )
    
    # Password protection for admin panel
    if "admin_authenticated" not in st.session_state:
        st.session_state.admin_authenticated = False
    
    if not st.session_state.admin_authenticated:
        st.title("üîê Admin Panel - Authentication Required")
        
        admin_password = st.text_input("Enter Admin Password:", type="password")
        
        if st.button("Login"):
            correct_password = os.getenv("ADMIN_PASSWORD") or st.secrets.get("ADMIN_PASSWORD", "admin123")
            
            if admin_password == correct_password:
                st.session_state.admin_authenticated = True
                st.success("Authentication successful! Redirecting...")
                st.rerun()
            else:
                st.error("Incorrect password. Please try again.")
        
        st.info("This is the admin panel for managing Aniket's portfolio assistant. Access is restricted to authorized users only.")
        return
    
    # Custom CSS for admin dashboard
    st.markdown("""
    <style>
    .admin-header {
        text-align: center; 
        padding: 25px; 
        background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%); 
        color: white; 
        border-radius: 15px; 
        margin-bottom: 30px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #2c3e50;
        margin: 10px 0;
        text-align: center;
    }
    .status-section {
        background: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        margin: 15px 0;
    }
    .danger-zone {
        background: #fff5f5;
        border: 2px solid #feb2b2;
        padding: 20px;
        border-radius: 10px;
        margin: 20px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Admin header
    st.markdown("""
    <div class="admin-header">
        <h1>‚öôÔ∏è Aniket Shirsat Portfolio - Admin Dashboard</h1>
        <p style="margin: 0; opacity: 0.9;">Complete Management & Analytics Interface</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Logout button in sidebar
    with st.sidebar:
        if st.button("üö™ Logout", type="secondary"):
            st.session_state.admin_authenticated = False
            st.rerun()
        
        st.markdown("---")
        st.markdown("### üéØ Quick Actions")
        
        # Show database status in sidebar
        st.markdown("### üîó Database Status")
        show_database_status()
        
    # Main admin interface - UPDATED WITH NEW TABS
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "üìä Analytics Dashboard", 
        "üìÑ Resume Management", 
        "üñºÔ∏è Avatar Management", 
        "üåê Website Scraping", 
        "‚öôÔ∏è System Settings",
        "üì° Live Monitoring",
        "üí¨ Complete Conversations"  # NEW TAB
    ])
    
    # Tab 1: Enhanced Analytics Dashboard
    with tab1:
        enhanced_analytics_tab_v2()
            
    # Tab 2: Resume Management
    with tab2:
        st.header("üìÑ Resume Management")
        
        # Current resume status
        resume_data = load_resume_shared()
        
        if resume_data:
            st.success("‚úÖ Resume Currently Loaded")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Filename", resume_data['filename'])
            with col2:
                st.metric("Word Count", resume_data['metadata']['word_count'])
            with col3:
                st.metric("File Size", f"{resume_data['metadata']['file_size'] / 1024:.1f} KB")
            with col4:
                upload_date = datetime.fromisoformat(resume_data['uploaded_at']).strftime("%m/%d/%Y")
                st.metric("Upload Date", upload_date)
            
            # Preview content
            st.subheader("üìÑ Content Preview")
            preview_text = resume_data['content'][:1000] + "..." if len(resume_data['content']) > 1000 else resume_data['content']
            st.text_area("Resume Content (First 1000 characters)", preview_text, height=200, disabled=True)
            
            # Management actions
            st.subheader("üîß Management Actions")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üóëÔ∏è Remove Current Resume", type="secondary"):
                    if delete_resume_shared():
                        st.success("Resume removed successfully!")
                        st.rerun()
                    else:
                        st.error("Failed to remove resume")
            
            with col2:
                st.markdown("**Resume is now active in chat widget!**")
                st.info("‚úÖ Chat widget will use this resume for answering questions")
        
        else:
            st.warning("‚ö†Ô∏è No resume currently loaded")
        
        st.markdown("---")
        
        # Upload new resume
        st.subheader("üì§ Upload New Resume")
        resume_file = st.file_uploader(
            "Choose Resume File",
            type=['pdf', 'docx', 'txt'],
            help="Supported formats: PDF, DOCX, TXT"
        )
        
        if resume_file:
            if st.button("üìù Process & Save Resume", type="primary"):
                process_resume_file(resume_file)
    
    # Tab 3: Avatar Management
    with tab3:
        st.header("üñºÔ∏è Avatar Management")
        
        saved_avatar = load_avatar_shared()
        
        if saved_avatar:
            st.success("‚úÖ Custom Avatar Currently Active")
            
            col1, col2 = st.columns([1, 3])
            with col1:
                st.markdown(f'<img src="{saved_avatar}" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover; border: 3px solid #2c3e50;">', unsafe_allow_html=True)
            
            with col2:
                st.markdown("**Current Avatar Details:**")
                st.write("- Format: Base64 PNG")
                st.write("- Size: 100x100 pixels")
                st.write("- Status: Active in both chat widget and admin")
                st.success("‚úÖ Avatar is synced between both applications!")
                
                if st.button("üóëÔ∏è Remove Current Avatar", type="secondary"):
                    if delete_avatar_shared():
                        st.success("Avatar removed successfully!")
                        st.rerun()
                    else:
                        st.error("Failed to remove avatar")
        
        else:
            st.info("‚ÑπÔ∏è No custom avatar currently set. Using default avatar.")
        
        st.markdown("---")
        
        # Upload new avatar
        st.subheader("üì§ Upload New Avatar")
        avatar_file = st.file_uploader(
            "Choose Avatar Image",
            type=['png', 'jpg', 'jpeg'],
            help="Recommended: Square image, at least 100x100 pixels"
        )
        
        if avatar_file:
            st.subheader("üîç Preview")
            preview_avatar = get_image_base64(avatar_file)
            if preview_avatar:
                col1, col2 = st.columns([1, 3])
                with col1:
                    st.markdown(f'<img src="{preview_avatar}" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover; border: 3px solid #2c3e50;">', unsafe_allow_html=True)
                with col2:
                    st.write("**Preview of uploaded avatar**")
                    if st.button("üíæ Save This Avatar", type="primary"):
                        if save_avatar_shared(preview_avatar):
                            st.success("‚úÖ Avatar saved and synced to chat widget!")
                            st.rerun()
                        else:
                            st.error("Failed to save avatar")
    
    # Tab 4: Website Scraping
    with tab4:
        st.header("üåê Website Content Management")
        
        st.subheader("üîß Scraping Configuration")
        
        col1, col2 = st.columns(2)
        with col1:
            website_url = st.text_input(
                "Portfolio Website URL",
                value="https://aniketdshirsat.com/",
                help="URL of Aniket's portfolio website"
            )
        
        with col2:
            max_pages = st.slider(
                "Maximum Pages to Analyze",
                min_value=1,
                max_value=50,
                value=10,
                help="Number of pages to scrape from the website"
            )
        
        # Scraping actions
        st.subheader("üöÄ Update Knowledge Base")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ Update Website Content", type="primary"):
                scrape_website(website_url, max_pages)
        
        with col2:
            if st.button("üß™ Test Single Page", type="secondary"):
                with st.spinner("Testing single page scrape..."):
                    scraper = SimpleWebsiteScraper()
                    content = scraper.scrape_page(website_url)
                    if content:
                        st.success(f"‚úÖ Successfully scraped: {content.title}")
                        st.write(f"Content length: {len(content.content)} characters")
                        st.write(f"Word count: {content.metadata['word_count']}")
                    else:
                        st.error("‚ùå Failed to scrape the page")
        
        # Advanced settings
        with st.expander("‚öôÔ∏è Advanced Scraping Settings"):
            st.info("Coming soon: Custom selectors, content filters, and scheduling options")
    
    # Tab 5: System Settings
    with tab5:
        st.header("‚öôÔ∏è System Settings & Configuration")
        
        # Database Connection Status
        st.subheader("üîó Database Connection Status")
        status = show_database_status()
        
        if status["connected"]:
            st.success("üéâ Apps are successfully connected and syncing data!")
            
            # Test data sync
            st.subheader("üß™ Test Data Synchronization")
            if st.button("üîÑ Test Sync"):
                # Test by reading current data
                avatar_test = load_avatar_shared()
                resume_test = load_resume_shared()
                user_test = load_user_data_shared()
                
                st.write("**Sync Test Results:**")
                st.write(f"‚úÖ Avatar: {'Found' if avatar_test else 'Not found'}")
                st.write(f"‚úÖ Resume: {'Found' if resume_test else 'Not found'}")
                st.write(f"‚úÖ User data: {len(user_test)} records")
                st.success("All data is accessible from shared database!")
        
        else:
            st.error("‚ùå Apps are not connected! Data changes won't sync.")
            st.markdown("""
            **To connect your apps:**
            1. Create a GitHub Gist at [gist.github.com](https://gist.github.com)
            2. Create a GitHub token with 'gist' permissions
            3. Add both to secrets in BOTH apps:
            ```
            GITHUB_TOKEN = "your-token"
            GIST_ID = "your-gist-id"
            ```
            """)
        
        # API Configuration
        st.subheader("üîë API Configuration")
        
        openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
        if openai_api_key:
            masked_key = openai_api_key[:7] + "..." + openai_api_key[-4:]
            st.success(f"‚úÖ OpenAI API Key configured: {masked_key}")
        else:
            st.error("‚ùå OpenAI API Key not found in environment variables")
        
        # App URLs
        st.subheader("üöÄ Deployment Information")
        
        st.info("""
        **Your App URLs:**
        - **Chat Widget**: Use for embedding in your portfolio website
        - **Admin Dashboard**: Keep private for management
        
        **Embedding Code:**
        ```html
        <iframe 
            src="https://mystrapp.vesselperform.com/" 
            width="420" 
            height="650" 
            frameborder="0"
            style="border-radius: 20px;">
        </iframe>
        ```
        """)
        
        # Data management
        st.subheader("üìä Data Management")
        
        # Export all data
        if st.button("üì• Export All Data"):
            db = get_shared_db()
            all_data = db.export_all_data()
            
            json_data = json.dumps(all_data, indent=2, default=str)
            st.download_button(
                label="üì• Download Complete Database Export",
                data=json_data,
                file_name=f"chatbot_database_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                help="Download complete database backup"
            )
        
        # Danger zone
        st.markdown("""
        <div class="danger-zone">
            <h3>‚ö†Ô∏è Danger Zone</h3>
            <p>These actions cannot be undone. Use with caution.</p>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üóëÔ∏è Clear All User Data", type="secondary"):
                if st.checkbox("I confirm I want to delete all user data"):
                    db = get_shared_db()
                    # Clear only user interactions
                    data = db._load_gist_data()
                    data["user_interactions"] = []
                    if db._save_gist_data(data):
                        st.success("All user data cleared successfully!")
                    else:
                        st.error("Failed to clear user data")
        
        with col2:
            if st.button("üí• Reset All Data", type="secondary"):
                if st.checkbox("I confirm I want to reset everything"):
                    db = get_shared_db()
                    if db.clear_all_data():
                        st.success("All data reset successfully!")
                        st.rerun()
                    else:
                        st.error("Failed to reset data")

    # Tab 6: Live Monitoring
    with tab6:
        live_monitoring_tab()

    # Tab 7: Complete Conversations - NEW TAB
    with tab7:
        conversation_threads_tab()
        st.markdown("---")
        export_conversation_threads()

if __name__ == "__main__":
    main()
